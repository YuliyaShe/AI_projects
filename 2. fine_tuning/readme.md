# Fine-Tuning моделей с использованием Unsloth

Этот проект демонстрирует процесс настройки (**fine-tuning**) современных языковых моделей с использованием библиотеки **Unsloth**. Для оптимизации работы моделей, таких как **Llama**, **Vicuna**, **Mistral** и других, с целью улучшения их производительности в специфичных задачах — например, в области **поддержки психического здоровья** или **разговорных ассистентов**.

## Основные возможности проекта

### Поддержка современных моделей
- **Llama**, **Mistral**, **Phi-3**, **Gemma**, **Yi**, **DeepSeek**, **Qwen**, **TinyLlama**, **Vicuna**, **Open Hermes** и другие.
- Возможность работы с:
  - **16-битным LoRA** (полноценное fine-tuning с низким рангом),
  - **4-битным QLoRA** (квантованная адаптация для экономии памяти и ускорения).

### Гибкость настройки
- Задание **максимальной длины последовательности** (`max_seq_length`) с поддержкой **автоматического масштабирования RoPE**.
- Поддержка длинных контекстов (до **342K токенов** для Llama 3.1 8B на 80 ГБ VRAM).

### Оптимизация производительности
- Ускорение обучения до **2×** по сравнению с Hugging Face + FlashAttention-2.
- Например, **Phi-3 Medium/Mini** обучается в **2 раза быстрее** благодаря оптимизированным ядрам Unsloth.

---

## Подготовка данных

Для обучения используется предварительно обработанный набор данных:

> **[Mental Health Counseling Conversations](https://huggingface.co/datasets/Amod/mental_health_counseling_conversations)**

Данный датасет совместим с библиотекой `datasets` от Hugging Face и содержит реальные диалоги между консультантами и клиентами.

### Пример кода для загрузки:

```python
from datasets import load_dataset

data = load_dataset("Amod/mental_health_counseling_conversations")
 ```
## Установка

Устанавливаем библиотеку **Unsloth** с помощью следующих команд:

```bash
# Стандартная установка (рекомендуется для большинства пользователей)
pip install unsloth
  ```

Убеждаемся, что установлены дополнительные зависимости для работы с моделями (например, `datasets`, `trl`, `accelerate`, `peft` и `bitsandbytes`).

## Настройка модели

### Добавление LoRA для обновления весов

**LoRA** (*Low-Rank Adaptation*) — это метод, позволяющий обновлять только низкоранговые матрицы вместо всех весов модели. Это значительно снижает требования к видеопамяти и ускоряет обучение без существенной потери качества.

> ⚠️ **Примечание**: В библиотеке Unsloth **нет класса `ModelTrainer`**. 
#### Пример кода:

```python
from unsloth import ModelTrainer

trainer = ModelTrainer(model="llama",
                       dataset="Amod/mental_health_counseling_conversations",
                       method="lora")

trainer.train()
```

## Параметры обучения

- **Метод обучения**: LoRA или QLoRA  
- **Параметры модели**: Возможность установки максимальной длины последовательности (`max_seq_length`) с поддержкой автоматической оптимизации (включая RoPE scaling для длинных контекстов).

## Преимущества

- **Экономия ресурсов**:  
  Использование методов **LoRA** и **QLoRA** позволяет значительно снизить потребление видеопамяти и ускорить обучение — до **70–80% меньше VRAM** и в **2 раза быстрее**, чем стандартные подходы.

- **Масштабируемость**:  
  Поддержка широкого спектра моделей (Llama, Mistral, Phi-3, Gemma, Qwen, DeepSeek и др.) и автоматическое масштабирование контекста (до **342K токенов** для Llama 3.1 8B).

- **Удобство использования**:  
  Простая интеграция с популярными библиотеками, такими как **Hugging Face Datasets**, **TRL**, **transformers**, а также экспорт в **GGUF**, **Ollama**, **vLLM** и другие форматы.

## Возможные улучшения

- Добавить поддержку ещё большего числа архитектур и методов оптимизации (например, **full-finetuning** с FFT, **multi-GPU**).
- Улучшить документацию и предоставить больше готовых шаблонов для разных доменов (медицина, психология, ритейл и др.).
- Интеграция с инструментами оценки качества моделей (например, **lm-eval**, **arena-hard**, **trl-eval**).



